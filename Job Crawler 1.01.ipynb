{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Masterlist [[JobPageItemList], [JobPageItemList], ... ]\n",
    "masterList = []\n",
    "counter = 0\n",
    "\n",
    "\n",
    "while counter < 30: # !!!!!! <----------------------- How many pages to you want to crawl (numver x 10) ??? TYPE IN HERE!!!!!\n",
    "    # Indeed.com Jobs in Hamburg sorted by date\n",
    "    url = \"https://de.indeed.com/Jobs?l=hamburg&sort=date&start=\" + str(counter) \n",
    "\n",
    "    indeedJobsHamburg = urlopen(url)\n",
    "    indeedJobsHamburg = BeautifulSoup(indeedJobsHamburg, \"html.parser\")\n",
    "    jobColumn = indeedJobsHamburg.find(id=\"resultsCol\") \n",
    "    jobDivs = jobColumn.find_all(class_=\"jobsearch-SerpJobCard unifiedRow row result\")\n",
    "\n",
    "    # List of Job Links Page 1 \n",
    "    linkList = []\n",
    "    for item in jobDivs:\n",
    "        jobHref = item.find(\"a\").attrs['href']\n",
    "        jobLink = \"https://de.indeed.com\" + jobHref\n",
    "        linkList.append(jobLink)\n",
    "\n",
    "    # Get links out of linkList, crawl through jobPage\n",
    "    for linkJobPage in linkList:    \n",
    "\n",
    "        jobPage = urlopen(linkJobPage)\n",
    "        jobPage = BeautifulSoup(jobPage, \"html.parser\")    \n",
    "\n",
    "        # Collect items of jobPage here // [JobPageItemList]   \n",
    "        jobPageItemList = []\n",
    "\n",
    "        # ListItems\n",
    "        publishDate = jobPage.find(class_=\"jobsearch-JobMetadataFooter\").get_text()\n",
    "        jobPageItemList.append(publishDate)\n",
    "\n",
    "        company = jobPage.find(class_=\"icl-u-lg-mr--sm icl-u-xs-mr--xs\").get_text()\n",
    "        jobPageItemList.append(company)\n",
    "\n",
    "        title = jobPage.find(\"title\").get_text()\n",
    "        jobPageItemList.append(title)\n",
    "\n",
    "        jobDescriptionText = jobPage.find(id=\"jobDescriptionText\").get_text()\n",
    "\n",
    "        wordCount = len(jobDescriptionText)\n",
    "        jobPageItemList.append(wordCount)\n",
    "        jobPageItemList.append(url)\n",
    "\n",
    "        # Add jobPageItemList to masterList\n",
    "        masterList.append(jobPageItemList)\n",
    "        print (jobPageItemList)\n",
    "\n",
    "    counter += 10\n",
    "print (\" \")\n",
    "# print (masterList)\n",
    "\n",
    "df0 = pd.DataFrame(np.array(masterList),\n",
    "                   columns=['Date', 'Company', 'JobTitle', 'Wordcount', 'Link'])\n",
    "\n",
    "df0\n",
    "\n",
    "# TODO NEXT\n",
    "# Two spereate Processes   1. Save all HTML pages in one folder    2. Pick these pages and analyse them\n",
    "# Why? Because if I want to change my analysy later, I can re-analyse everything later\n",
    "\n",
    "# Clean 'Date' Column \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
